---
layout: single
title:  "텍스트 데이터 분석" 
categories : 
    - Python
---
## Tokenizing

1. Tokenize
    ```python
    import nltk # Natural Language TooKit
        token = nltk.word_tokenize(sentence) # 문장을 토큰(단어)화하여 리스트 형태로 만듬

        nltk.post_tag(token) # 토큰에 명사, 형용사 등 품사별로 구분해주고 짝지어줌
    ```

2. Token에서 Stop_words(불용어) 제거
    1. 불용어 사전 불러오기

    ```python
    from nltk.corpus import stopwords

    stopWords = stopwords.words.('english') # 영어에서 문장 내에 의미 없는 불용어를 불러오는 함수로 print하게 되면 큰 의미 없는 말들이 나옴

    stopwords.fileids() # 가능한 언어 어떤 것이 있는지 알려주는 함수
    ```

    2. 불용어 사전은 모두 소문자로 되어있어 Token도 소문자로 전환하기
        * Stop_words는 Customizing 가능 
    ```python
    result = []

    for token in tokens:

        if token.lower not in stopWords:
                result.append(token)
    ```

3. Lemmatizing (단어의 기본 사전화)
    * nltk.wordnet.WordNetLemmatizer() Str, 품사를 넣으면 lamma를 출력하는 함수
    * lemmatizer와 pos_tag를 for문에 담아 돌리면 사전형을 불러옴 