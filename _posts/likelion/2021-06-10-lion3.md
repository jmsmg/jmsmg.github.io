---
layout: single
title:  "웹 크롤링"
categories : 
    - Python
---
## 웹 크롤링

---

1. 크롤링 과정
    1. URL 분석 (패턴 존재 여부, query 종류)
       * URL 긁어오기 -> query 분석

    2. URL 구성 (변수화 -> str, 자동화 고려)
        * URL 분석한 것을 나누어서 쿼리 분류작업

            ```python
                word = '보람튜브' 
                URL = "https://search.naver.com/search.naver?where=news&query=" + word
            ```

    3. HTTP Response 얻기
        * URL에서 내용 뽑아내기 // 뽑아낸 상태는 사람이 볼 수 없는 자료형태로 가공할 준비해야함

            ```python
                web = requests.get(URL).content # 또는 urlopen(URL) 함수를 통해서 변수화
                # urlopen함수는 ascii코드여서 한글이 깨져서 보람튜브 같은 쿼리를 넣을 수 없게 됨
            ```

    4. HTML source 얻기
        * BeatifulSoup를 통해 parsing작업을 진행해야함
            > BeatifulSoup(HTTP Response, 'html.parser')

            ``` python
            source = BeautifulSoup(web, 'html.parser')
            ```

    5. HTML Tag 꺼내기
        * 현재 HTML형태로 태그, 셀렉터가 모두 나왔으므로 필요한 정보(제목, 기사별URL 등)만 뽑아야함
            <details>
            <summary>find, find_all 함수 펼치기</summary>
            <div markdown="1">
                        * .find('Tag이름', {'Attr 이름':'Attr 값'}) 1개의 Tag (조건이 동일한 Tag가 여러개면 첫번째만 꺼내줌)
                            > 바로 위 부모 태그를 감싸서 대입 시킴

                            ``` python
                            box1 = web_page.find('ul', {"class" : "list_search"}).get_text().replace('\n','')
                            ```

                        * .find_all(~) : 여러개의 Tag를 찾은 다음 for문으로 Tag 단위로 꺼내서 활용  
                            > 직접 태그를 감싸서 대입시키고 for문으로 get_text로 뽑아냄
            </div>
            </details>

        * 태그끼리 묶어내기

            ``` python
            news_subjects = source.find_all('a', {'class' : 'news_tit'}) 
            # 전체 HTML content에서 a태그 목록으로 전환하는 과정으로 a태그 끼리 묶어줌
                # a태그로 이루어진 ResultSet(=list)이 완성됨
            ```
            
        * 텍스트만 뽑아내기
            ```python
            subject_list = []
                
                # subject를 변수로두어 = a 태그 1개

            for subject in news_subjects:
                subject_list.append(subject.get_text())

                print(subject_list)
                # find_all로 꺼내면 list
                print(span.get_text())  # ResultSet의 함수에는 get_text() 함수가 존재하지 않음
            ```

        * URL만 뽑아내기
            ```python
            urls = news_subjects

            first_article = url[0]
            first_article.attrs['href']

            for urls in source.find_all('a', {'class' : 'news_tit'}):
                print(first_article.attrs['href'])
            ```

    6. 링크 안에 들어가서 기사 본문 뽑아내기
        * 출처(신문사)마다 모두 하나씩 본문 지정해줘서 뽑아내는 작업을 해줘야함 // 아니면 별도의 방법 생각
        * 네이버의 경우 네이버뉴스가 따로 지정되어 있어 이것으로 뽑아냄
            ```python
            for urls in source.find_all('a', {'class' : 'info'}):  # 네이버 뉴스 버튼이 class="info"
                print(urls.attrs['href'])
                # 언론사 이름도 class="info"였음 -> 짝수 홀수로 뽑아 봤지만 불완전함
            ```
        
        * 네이버 뉴스의 주소(news.naver.com/)를 이용해서 startswith()함수(시작하는 문자가 맞는지 확인하는 함수) 사용
            ``` python
            urls_list = []

            for urls in source.find_all('a', {'class' : "info"}): # 전체를 뽑아내는 for문
                if urls.attrs["href"].startswith("https://news.naver.com"): # True면 뽑아내는 조건문

                    urls_list.append(urls.attrs["href"]) #urls_list[]에 href 부분을 넣어주는 부분
            urls_list # 완료된 urls_list 호출
            ```
    7. 단일 뉴스페이지 분석하기
        * 
    8. Tag로 부터 텍스트 혹은 Atrribute values 꺼내기 : Tag.get_text() or Tag.attrs
        ```python
            with open('brunch.txt','a',encoding = 'utf-8') as f: # 자동으로 닫아주는 코드 mode: 'a'
                all_text = source.find('div',{'class': 'wrap_body'}) # class wrp_body 하나만 가져옴
                article = all_text.find_all('p') # 부모 태그 안에 모든 'p' 태그를 가져와줌
        
            for content in article:
                print(content.get_text())
                f.write(content.get_text() + '\n')
2. 