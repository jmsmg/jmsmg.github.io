## 웹 크롤링

---

1. 크롤링 과정
    * url 분석 (패턴 존재 여부, query 종류)
    * url 구성 (변수화 -> str, 자동화 고려)  # query 확인 
        * url = 'https://alldic.daum.net/search.do?q=' + word
    * HTTP Response 얻기 : (urlopen(URL) or request.get(URL).content)
        * urlopen(URL) 
        ``` python 
        <http.client.HTTPResponse at 0x7f93d15ff0a0>
        ```
    * HTML source 얻기 : BeatifuleSoup(HTTP Response, 'html.parser')  
        ``` python
        web_page = BeautifulSoup(web, 'html.parser')
        ```
    * HTML Tag 꺼내기 : .find('Tag이름', {'Attr 이름':'Attr 값'}) 
        * .find(~) : 1개의 Tag (조건이 동일한 Tag가 여러개면 첫번째만 꺼내줌)
            * 바로 위 부모 태그를 감싸서 대입 시킴
                ``` python
                box1 = web_page.find('ul', {"class" : "list_search"}).get_text().replace('\n','')
                ```

        * .find_all(~) : 여러개의 Tag를 찾은 다음 for문으로 Tag 단위로 꺼내서 활용  
            * 직접 태그를 감싸서 대입시키고 for문으로 get_text로 뽑아냄
                ``` python
                box2 = web_page.find_all('span', {'class': 'txt_search'})# 모두 찾다
                for span in box2: # find_all로 꺼내면 list
                    print(span.get_text())  # ResultSet의 함수에는 get_text() 함수가 존재하지 않음
                ```
    * Tag로 부터 텍스트 혹은 Atrribute values 꺼내기 : Tag.get_text() or Tag.attrs
        ```python
            with open('brunch.txt','a',encoding = 'utf-8') as f: # 자동으로 닫아주는 코드 mode: 'a'
                all_text = source.find('div',{'class': 'wrap_body'}) # class wrp_body 하나만 가져옴
                article = all_text.find_all('p') # 부모 태그 안에 모든 'p' 태그를 가져와줌
        
            for content in article:
                print(content.get_text())
                f.write(content.get_text() + '\n')
2. 